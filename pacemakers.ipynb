{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacemaker identification with neural networks\n",
    "This is a sample notebook to go with the pacemaker dataset.\n",
    "This is the dataset used by the paper [\"Cardiac Rhythm Device Identification Using Neural Networks\"](http://electrophysiology.onlinejacc.org/content/5/5/576)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import deque\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "##  Settings for our dataset\n",
    "We use the mean and standard deviation of the ImageNet dataset so our images are of similar distribution to the pre-trained models we'll load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./Train\"\n",
    "TEST_DIR = \"./Test\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20  # Go through the entire dataset 20 times during training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 if torch.cuda.is_available() and torch.cuda.get_device_name() == \"Quadro P1000\" else 32\n",
    "VERBOSE = True  # Print progress of each training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our data pipeline\n",
    "\n",
    "For training examples we'll use data augmentation to distort the images and make them look a bit different, so the neural network sees more examples, effectively.\n",
    "For the testing set, we won't adulterate them (so we can judge a more 'real world' performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.RandomAffine(degrees=5,\n",
    "                            translate=(0.05, 0.05),\n",
    "                            scale=(0.95, 1.05),\n",
    "                            shear=5),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=transforms_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(TEST_DIR, transform=transforms_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "n_classes = len(train_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))  # Larger plot size\n",
    "\n",
    "img = train_data[0][0].numpy().transpose((1, 2, 0))\n",
    "img = STD * img + MEAN\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Training set example\")\n",
    "\n",
    "img = test_data[0][0].numpy().transpose((1, 2, 0))\n",
    "img = STD * img + MEAN\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Testing set example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our network\n",
    "We'll use DenseNet121 (Xception used in the original paper isn't in the Pytorch model zoo, sadly, and DenseNet is still very nice).\n",
    "Because this network will have been trained on ImageNet which has 1000 classes, and we are training on our pacemakers which have 45 classes, we need to replace the final layer of the network with a layer with 45 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet121(pretrained=True)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, n_classes)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training scheme\n",
    "Here we tell it we want it to calculate its performance using CrossEntropyLoss (because it's a categorical problem).\n",
    "We're going to use the Ignite framework here just to make our training loops a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "trainer = create_supervised_trainer(model,\n",
    "                                    optimizer,\n",
    "                                    loss,\n",
    "                                    device=DEVICE)\n",
    "\n",
    "evaluator = create_supervised_evaluator(model,\n",
    "                                        metrics={'accuracy': Accuracy(),\n",
    "                                                 'loss': Loss(loss),\n",
    "                                                 'precision': Precision()},\n",
    "                                        device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some functions to print out our progress\n",
    "These are called 'callbacks', or 'hooks'. Ignite will run these funcstions when certain things happen, e.g. end of an epoch (every cycle, i.e. time the network's been trained a full copy of the dataset), or every iteration (every 'batch' of pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.STARTED)\n",
    "def initialise_custom_engine_vars(engine):\n",
    "    engine.iteration_timings = deque(maxlen=100)\n",
    "    engine.iteration_loss = deque(maxlen=100)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    engine.iteration_timings.append(time.time())\n",
    "    engine.iteration_loss.append(engine.state.output)\n",
    "    seconds_per_iteration = np.mean(np.gradient(engine.iteration_timings)) if len(engine.iteration_timings) > 1 else 0\n",
    "    eta = seconds_per_iteration * (len(train_loader)-(engine.state.iteration % len(train_loader)))\n",
    "    if VERBOSE:\n",
    "        print(f\"\\rEPOCH: {engine.state.epoch:03d} | \"\n",
    "              f\"BATCH: {engine.state.iteration % len(train_loader):03d} of {len(train_loader):03d} | \"\n",
    "              f\"LOSS: {engine.state.output:.3f} ({np.mean(engine.iteration_loss):.3f}) | \"\n",
    "              f\"({seconds_per_iteration:.2f} s/it; ETA {str(datetime.timedelta(seconds=int(eta)))})\", end='')\n",
    "            \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    acc, loss, precision = metrics['accuracy'], metrics['loss'], metrics['precision'].cpu()\n",
    "    print(f\"\\nEnd of epoch {engine.state.epoch:03d}\")\n",
    "    print(f\"TRAINING Accuracy: {acc:.3f} | Loss: {loss:.3f}\")\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    acc, loss, precision = metrics['accuracy'], metrics['loss'], metrics['precision'].cpu()\n",
    "    print(f\"TESTING  Accuracy: {acc:.3f} | Loss: {loss:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have achieved a respectable accuracy of over 90% on the testing set at some stages, which is good considering there are 45 classes.\n",
    "\n",
    "You may well see accuracies above those reported in our paper when you train this network - one reason for this is because in our paper we did not continuously measure performance against the testing set during training (we used a proportion of the training set to do this), but only once at the end. This the 'correct' practice, because it prevents \"lucky\" runs being reported as the true accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some predictions to test our network\n",
    "Here we will take an example of each of the classes in the testing dataset and run it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "plt.figure(figsize=(20,50))  # Larger plot size\n",
    "\n",
    "for i_class in range(n_classes):\n",
    "    \n",
    "    i_img = i_class * 5  # 5 examples per class\n",
    "    img_tensor, _ = test_data[i_img]\n",
    "    img_numpy = img_tensor.numpy().transpose((1, 2, 0))\n",
    "    img_numpy = STD * img_numpy + MEAN\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.unsqueeze(img_tensor, 0).to(DEVICE))\n",
    "        predicted_class = torch.argmax(predictions).cpu().numpy()\n",
    "    \n",
    "    true_class = test_data.classes[i_class][:20]\n",
    "    pred_class = test_data.classes[predicted_class][:20]\n",
    "    correct = \"CORRECT\" if true_class == pred_class else \"INCORRECT\"\n",
    "    \n",
    "    plt.subplot(9, 5, i_class+1)\n",
    "    plt.imshow(img_numpy)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{correct}\\nTrue class: {true_class}\\nPredicted class: {pred_class}\")\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pacemakers]",
   "language": "python",
   "name": "conda-env-pacemakers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
